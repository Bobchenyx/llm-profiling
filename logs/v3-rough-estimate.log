
Loading config from deepseek-ai/DeepSeek-V3...

DeepSeek-V3 Architecture Summary:
   Layers: 61 (3 Dense + 58 MoE)
   Hidden size: 7168
   Attention heads: 128 (Q), 128 (KV)
   Head dims: 192 (Q/K), 128 (V)
   Q/K/V low-rank: 1536 / 512 / 512
   MoE config: 256 routed experts, top-8, 1 shared
   Vocab size: 129,280

===========================================================================
Analyzing PREFILL phase...
===========================================================================

===========================================================================
PREFILL (B=4, L=128)
===========================================================================
Component                           TFLOPs       %        GFLOPs/tok  
---------------------------------------------------------------------------
moe_routed_experts                      20.925    56.01%       40.869
attention_output_proj                    7.336    19.64%       14.328
attention_qkv_proj                       4.814    12.89%        9.403
moe_shared_experts                       2.616     7.00%        5.109
dense_mlp                                1.218     3.26%        2.378
attention_compute                        0.330     0.88%        0.645
moe_router                               0.109     0.29%        0.213
lm_head                                  0.007     0.02%        0.014
mlp_pointwise                            0.003     0.01%        0.006
rmsnorm                                  0.002     0.00%        0.003
---------------------------------------------------------------------------
TOTAL                                   37.359   100.00%       72.968
===========================================================================

===========================================================================
Analyzing DECODE phase (first step)...
===========================================================================

===========================================================================
DECODE Step 1 (B=4, KV=128)
===========================================================================
Component                           TFLOPs       %        GFLOPs/tok  
---------------------------------------------------------------------------
moe_routed_experts                       0.163    54.63%       40.869
attention_output_proj                    0.057    19.15%       14.328
attention_qkv_proj                       0.038    12.57%        9.403
moe_shared_experts                       0.020     6.83%        5.109
dense_mlp                                0.010     3.18%        2.378
lm_head                                  0.007     2.48%        1.853
attention_compute                        0.003     0.87%        0.650
moe_router                               0.001     0.28%        0.213
mlp_pointwise                            0.000     0.01%        0.006
rmsnorm                                  0.000     0.00%        0.003
---------------------------------------------------------------------------
TOTAL                                    0.299   100.00%       74.812
===========================================================================

===========================================================================
Analyzing DECODE phase (step 128)...
===========================================================================

===========================================================================
DECODE Step 128 (B=4, KV=255)
===========================================================================
Component                           TFLOPs       %        GFLOPs/tok  
---------------------------------------------------------------------------
moe_routed_experts                       0.163    54.17%       40.869
attention_output_proj                    0.057    18.99%       14.328
attention_qkv_proj                       0.038    12.46%        9.403
moe_shared_experts                       0.020     6.77%        5.109
dense_mlp                                0.010     3.15%        2.378
lm_head                                  0.007     2.46%        1.853
attention_compute                        0.005     1.71%        1.289
moe_router                               0.001     0.28%        0.213
mlp_pointwise                            0.000     0.01%        0.006
rmsnorm                                  0.000     0.00%        0.003
---------------------------------------------------------------------------
TOTAL                                    0.302   100.00%       75.451
===========================================================================

===========================================================================
Computing TOTAL DECODE FLOPs for 128 steps...
===========================================================================
Linear interpolation:
   Step 1:      0.2992 TFLOPs
   Step 128:     0.3018 TFLOPs
   Average:     0.3005 TFLOPs/step
   Total:       38.47 TFLOPs

===========================================================================
KEY INSIGHTS
===========================================================================

1. MoE Contribution (Prefill):
   Router:            0.109 TFLOPs ( 0.29%)
   Routed experts:   20.925 TFLOPs (56.01%)
   Shared experts:    2.616 TFLOPs ( 7.00%)
   ───────────────────────────────
   MoE Total:        23.650 TFLOPs (63.30%)

2. Attention Compute Scaling:
   Prefill (L=128):                 0.330 TFLOPs ( 0.88%)
   Decode step 1 (KV=128):         0.003 TFLOPs ( 0.87%)
   Decode step 128 (KV=255):    0.005 TFLOPs ( 1.71%)
   Growth: 1.98x (decode step 1 -> 128)

3. Per-Token FLOPs:
   Prefill:           72.97 GFLOPs/token
   Decode step 1:     74.81 GFLOPs/token
   Decode step 128:    75.45 GFLOPs/token
   Growth: 1.01x (decode only)

4. Total FLOPs Summary:
   Prefill (128 tokens):          37.36 TFLOPs (49.27%)
   Decode (128 steps):           38.47 TFLOPs (50.73%)
   ───────────────────────────────────────────
   Grand Total:                   75.83 TFLOPs

===========================================================================

